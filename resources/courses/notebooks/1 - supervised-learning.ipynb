{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00000-f4e8a22f-e527-45b0-b2b8-89634840e6e4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.datasets import load_diabetes, load_digits\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, \\\n",
    "    Lasso, ElasticNet, Lars, LassoLars, OrthogonalMatchingPursuit, \\\n",
    "    BayesianRidge, ARDRegression, \\\n",
    "    RANSACRegressor, TheilSenRegressor, HuberRegressor, \\\n",
    "    LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # allow multiple cell output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-d9223aee-fc2f-4e21-bd54-c1b11a3a6b94"
   },
   "source": [
    "Linear Models\n",
    "============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-0c1f954e-054a-47b8-914a-9488c52aedd5"
   },
   "source": [
    "The following are a set of methods intended for regression in which the target value, $\\hat{y}$,  is expected to be a linear combination of the $p$ features $x_i$:\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$$  \n",
    "where $w = (w_1,..., w_p)$ are the `coef_` and $w_0$ the `intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00003-9b07d1bd-e985-493b-8482-ace51383e9ed",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Import diabetes & digits data as examples\n",
    "diabetes_X, diabetes_y = load_diabetes(return_X_y=True) #diabetes for regression\n",
    "digits_X, digits_y = load_digits(return_X_y=True) #digits for classification\n",
    "\n",
    "# Train-Test split\n",
    "diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = train_test_split(diabetes_X, diabetes_y, test_size=0.2, random_state=42)\n",
    "digits_X_train, digits_X_test, digits_y_train, digits_y_test = train_test_split(digits_X, digits_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00004-51b9f3ff-4d13-472a-b17c-e89cf97ad98f",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "reg_keys = [\"intercept\"] + [f\"w{i}\" for i in range(diabetes_X_train.shape[1])] + [\"R^2\"]\n",
    "reg_results = []\n",
    "reg_index = []\n",
    "\n",
    "clf_keys = [\"Accuracy\"]\n",
    "clf_results = []\n",
    "clf_index = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-74f41002-76a6-4301-a0e9-bf2ba4ff0bf3"
   },
   "source": [
    "As we know, scikit-learn *Estimator* object have a `fit` method, to learn from the model, used on the training data. It also have a `predict` method, usually used on test data after the training step, to predict target based on $X$ features. <br>\n",
    "\n",
    "It also have a `score` method, to evaluate the performance of the model for the task.  \n",
    "For a regression task, the metric used is usually the $R^{2}$, defined as follow:\n",
    "\n",
    "$$R^{2} = 1 - \\frac{\\text{Residual Sum of Squares}}{\\text{Total Sum of Squares}}$$\n",
    "where $\\text{Residual Sum of Squares} = \\sum{(y_{true} - y_{pred})^{2}}$ and $\\text{Total Sum of Squares} = \\sum{(y_{true} - \\frac{\\sum_{1}^{n}y_{true}}{n})^{2}}$.\n",
    "\n",
    "The best possible score is 1 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of $y$, disregarding the input features, would get a $R^{2}$ score of 0.\n",
    "\n",
    "For a classification task, it's the mean accuracy:\n",
    "$$ \\frac{\\text{# good prediction}}{n_{samples}}$$\n",
    "\n",
    "*Estimator* can also have a `set_params` and a `get_params` depending on the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-88699645-2681-490b-a1f7-4066467691f8"
   },
   "source": [
    "# 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-8d365c89-0ca4-46b2-8d37-2f6235f0af82"
   },
   "source": [
    "## 1.1 Classical linear regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-da599f35-232d-4bdd-946e-c903a2c67e02"
   },
   "source": [
    "### 1.1.1 Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-9691ff87-4e2c-47b3-9cfb-34edf191f2c0"
   },
   "source": [
    "LinearRegression fits a linear model with coefficients $w = (w_1,..., w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation: \n",
    "$$\\min_{w} || X w - y||_2^2$$\n",
    "\n",
    "To sum up, *OLS* is a strategy to obtain, from your model, a ‘straight line’ which is as close as possible to your data points. Even though *OLS* is not the only optimization strategy, it is the most popular for this kind of tasks, since the outputs of the regression (that are, coefficients) are unbiased estimators of the real values of `alpha` ($w_0$) and `beta`$(w_1,..., w_p)$. Indeed, according to the Gauss-Markov Theorem, under some assumptions of the linear regression model (linearity in parameters, random sampling of observations, conditional mean equal to zero, absence of multicollinearity, homoskedasticity of errors), the *OLS* estimators `α` and `β` are the Best Linear Unbiased Estimators (BLUE) of the real values of `α` and `β`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-00b55b9c-7660-4fbc-8434-0e9e793c05d1"
   },
   "source": [
    "<img src=\"../../resources/img/ols.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-6a42f944-d704-435c-9d57-140201116f30"
   },
   "source": [
    "LinearRegression will take in its `fit` method arrays $X$, $y$ and will store the coefficients  of the linear model in its `coef_` member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00012-0167b44d-680f-4c82-8e6e-f5ab8ae25a23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 151.3456553477407\n",
      "Coefficients: [  37.90031426 -241.96624835  542.42575342  347.70830529 -931.46126093\n",
      "  518.04405547  163.40353476  275.31003837  736.18909839   48.67112488]\n",
      "R^2: 0.45260660216173776\n"
     ]
    }
   ],
   "source": [
    "ols = LinearRegression()\n",
    "ols.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "ols_coef = ols.coef_\n",
    "ols_intercept = ols.intercept_\n",
    "\n",
    "ols_score = ols.score(diabetes_X_test, diabetes_y_test)\n",
    "\n",
    "print(f\"Intercept: {ols_intercept}\")\n",
    "print(f\"Coefficients: {ols_coef}\")\n",
    "print(f\"R^2: {ols_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00013-439eac31-63c0-4531-86c9-f4d3cbc9dddb"
   },
   "outputs": [],
   "source": [
    "reg_index.append(\"OLS\")\n",
    "reg_results.append(dict(zip(reg_keys, [ols_intercept] + ols_coef.tolist() + [ols_score])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-439d3e86-9d0b-4dca-90f1-223425b8064e"
   },
   "source": [
    "The coefficient estimates for *OLS* rely on the **independence of the features**. When features are correlated and the columns of $X$ have an approximate linear dependence, $X$ becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design.\n",
    "\n",
    "<u>Complexity</u>: The least squares solution is computed using the singular value decomposition of $X$. If $X$ is a matrix of shape `(n_samples, n_features)` this method has a cost of $O(n_{\\text{samples}} n_{\\text{features}}^2)$, assuming that $n_{\\text{samples}} \\geq n_{\\text{features}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-896adce4-20ba-4057-986b-2cd987b57a4a"
   },
   "source": [
    "[OLS explained visually](https://setosa.io/ev/ordinary-least-squares-regression/)\n",
    "\n",
    "*Ridge* and *Lasso* regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-98ee2783-7f2b-4ac9-8376-9ee3b301e72c"
   },
   "source": [
    "### 1.1.2 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-c43327ad-3051-4139-9da3-bd26ab8c046c"
   },
   "source": [
    "*Ridge* regression addresses some of the problems of *OLS* by imposing a **penalty on the size of the coefficients. Then it shrinks the coefficients and helps to reduce the model complexity and multi-collinearity**.<br>\n",
    "The ridge coefficients minimize a penalized residual sum of squares:\n",
    "$$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n",
    "The complexity parameter $\\alpha \\geq 0$ controls the amount of shrinkage: the larger the value of , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-8f48e42f-305e-430b-ad2c-3f1563a456ea"
   },
   "source": [
    "<img src=\"../../resources/img/ridge.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-6d541eca-628f-4dd3-8404-9335c8b156f4"
   },
   "source": [
    "<u>Complexity</u>: This method has the same order of complexity as OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-3cd463e2-1f9f-4e18-acf8-d06477e82359"
   },
   "source": [
    "## 1.2 Regressors with variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-531241c1-8c20-4db7-9374-cfcf7c1d632d"
   },
   "source": [
    "The following estimators have built-in variable selection fitting procedures, but any estimator using a $\\ell_1$ or *elastic-net* penalty also performs variable selection: typically *SGD* with an appropriate penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-2e0798be-2b50-4961-91b6-8ee42f1f7f69"
   },
   "source": [
    "### 1.2.1 Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-e34e10ab-0a8a-4efd-82fa-83bbf8666c86"
   },
   "source": [
    "The *Lasso* is a linear model that estimates sparse coefficients. It is useful in some contexts due to its **tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features** upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing. <br>\n",
    "It consists of a linear model with an added regularization term. The objective function to minimize is:\n",
    "$$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-68184e98-acdc-4e76-9fac-b9b3f2a597e0"
   },
   "source": [
    "The lasso estimate thus solves the minimization of the least-squares penalty with $\\alpha ||w||_1$ added, where $\\alpha ||w||_1$ is a constant and $||w||_1$ is the $\\ell_1$-norm of the coefficient vector.<br>\n",
    "The implementation in the class Lasso uses coordinate descent as the algorithm to fit the coefficients. <br>\n",
    "The `alpha` parameter controls the degree of sparsity of the estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00031-3fbd8f05-687e-4b90-84ff-ac5ab95c9c0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 151.57488311231123\n",
      "Coefficients: [   0.         -152.66706552  552.6941724   303.37055083  -81.3648345\n",
      "   -0.         -229.25829816    0.          447.91818931   29.64235375]\n",
      "R^2: 0.4718552616908692\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "lasso_coef = lasso.coef_\n",
    "lasso_intercept = lasso.intercept_\n",
    "\n",
    "lasso_score = lasso.score(diabetes_X_test, diabetes_y_test)\n",
    "\n",
    "print(f\"Intercept: {lasso_intercept}\")\n",
    "print(f\"Coefficients: {lasso_coef}\")\n",
    "print(f\"R^2: {lasso_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00032-80d2a5ed-39f6-4f1c-81e8-936272978d6f"
   },
   "outputs": [],
   "source": [
    "reg_index.append(\"Lasso\")\n",
    "reg_results.append(dict(zip(reg_keys, [lasso_intercept] + lasso_coef.tolist() + [lasso_score])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-93bc86c4-64e2-47e1-bdc7-02c324071777"
   },
   "source": [
    "As the Lasso regression yields sparse models, it **not only helps in reducing over-fitting but it can help us in feature selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00034-83feab04-fc83-4a26-948a-51c14faabc2e"
   },
   "source": [
    "The equivalence between `alpha` and the regularization parameter of *SVM*, `C` is given by $\\alpha = \\frac{1}{C}$ or $\\alpha = \\frac{1}{C \\cdot n_{samples}}$ depending on the estimator and the exact objective function optimized by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-390b230c-0902-4be8-864f-b858e5814efe"
   },
   "source": [
    "### 1.2.2 Elastic-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-815acc23-2315-4fba-89eb-540b91cf53e5"
   },
   "source": [
    "ElasticNet is a linear regression model trained with both $\\ell_1$ and $\\ell_2$-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like **Lasso**, while still maintaining the regularization properties of **Ridge**. We control the convex combination of $\\ell_1$ and $\\ell_2$ using the `l1_ratio` parameter.\n",
    "\n",
    "*Elastic-net* is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while *elastic-net* is likely to pick both.<br>\n",
    "A practical advantage of trading-off between *Lasso* and *Ridge* is that it allows *Elastic-Net* to inherit some of *Ridge*’s stability under rotation.<br>\n",
    "The objective function to minimize is in this case:\n",
    "$$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00037-8d98d1ca-acfa-4d9b-97d0-676b9c66b13a"
   },
   "source": [
    "<img src=\"../../resources/img/elastic-net.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00038-030661a4-8ea1-4cc3-83c7-c752297d0528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 153.71775135185501\n",
      "Coefficients: [ 0.40100381  0.          3.41353374  2.32617327  0.45957655  0.1255522\n",
      " -1.78311242  2.12401015  3.07138726  1.9061834 ]\n",
      "R^2: -0.0024651935487560728\n"
     ]
    }
   ],
   "source": [
    "elastic_net = ElasticNet()\n",
    "elastic_net.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "elastic_coef = elastic_net.coef_\n",
    "elastic_intercept = elastic_net.intercept_\n",
    "\n",
    "elastic_score = elastic_net.score(diabetes_X_test, diabetes_y_test)\n",
    "\n",
    "print(f\"Intercept: {elastic_intercept}\")\n",
    "print(f\"Coefficients: {elastic_coef}\")\n",
    "print(f\"R^2: {elastic_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00039-a8d6a5a1-48bd-46ce-9c7f-f348adf1492a"
   },
   "outputs": [],
   "source": [
    "reg_index.append(\"ElasticNet\")\n",
    "reg_results.append(dict(zip(reg_keys, [elastic_intercept] + elastic_coef.tolist() + [elastic_score])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-115a4389-89b7-4dca-a76f-493fa7c0d92e"
   },
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"../../resources/img/geometric-regul.png\" alt=\"Drawing\" width=\"500\"/> </td>\n",
    "<td> <img src=\"../../resources/img/l1-l2-elastic.png\" alt=\"Drawing\" width=\"500\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "With the decrease of t, both methods get smaller $\\beta_1$ and $\\beta_2$. Here comes the difference between L1 and L2: the tangent point for L1 cases is more likely to be achieved on the axis (some parameter be reduced to zero), while the tangent point for L2 cases is more likely to be got on a none-axis point (none zero points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00084-06c04b46-48ff-4ac8-a9b6-baac2f8524f4"
   },
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00085-ebd722db-6858-4f8b-8af1-63f47f5c1f10"
   },
   "source": [
    "### 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00086-9034d2dc-0c17-4abf-9a18-b0a2fed67545"
   },
   "source": [
    "`LogisticRegression`, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function. <br>\n",
    "This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional $\\ell_1$, $\\ell_2$ or Elastic-Net regularization. <br>\n",
    "Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting C to a very high value. <br>\n",
    "As an optimization problem, binary class $\\ell_2$ penalized logistic regression minimizes the following cost function:\n",
    "$$ \\min_{w, c} \\frac{1}{2}w^T w + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .$$\n",
    "Similarly, $\\ell_1$ regularized logistic regression solves the following optimization problem:\n",
    "$$\\min_{w, c} \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1).$$\n",
    "**Elastic-Net** regularization is a combination of $\\ell_1$ and $\\ell_2$ and minimizes the following cost function:\n",
    "$$\\min_{w, c} \\frac{1 - \\rho}{2}w^T w + \\rho \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1)$$\n",
    "where $\\rho$ controls the strength of $\\ell_1$ regularization vs. $\\ell_2$ regularization (it corresponds to the `l1_ratio` parameter).\n",
    "\n",
    "Note that, in this notation, it’s assumed that the target $y_i$ takes values in the set $[-1;1]$. We can also see that Elastic-Net is equivalent to $\\ell_1$ when $\\rho=1$ and equivalent to $\\ell_2$ when $\\rho=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00087-a0fc57b0-b391-41b4-b22e-d0a8026c617b"
   },
   "source": [
    "A logistic regression with $\\ell_1$ penalty yields sparse models, and can thus be used to perform feature selection, as detailed in **L1-based feature selection**.\n",
    "\n",
    "It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The `statsmodels` package natively supports this. Within sklearn, one could use bootstrapping instead as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00088-55ec1af4-c3c3-4a65-b4f5-2f02537e7d29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=10000)\n",
    "lr.fit(digits_X_train, digits_y_train)\n",
    "\n",
    "lr_score = lr.score(digits_X_test, digits_y_test)\n",
    "\n",
    "print(f\"Accuracy: {lr_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00089-ca5df064-48cb-4a88-91c2-9b19ec6cb604"
   },
   "outputs": [],
   "source": [
    "clf_index.append(\"LogisticRegression\")\n",
    "clf_results.append(dict(zip(clf_keys, [lr_score])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00090-84d0b2dc-1bc0-4f28-b4f6-6e4dea2dfd48"
   },
   "source": [
    "### 2.2 Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00091-09c18e4b-d588-4628-831d-715d432cb7eb"
   },
   "source": [
    "The *Perceptron* is another simple classification algorithm suitable for large scale learning. By default:\n",
    "\n",
    "It does not require a learning rate.\n",
    "\n",
    "It is not regularized (penalized).\n",
    "\n",
    "It updates its model only on mistakes.\n",
    "\n",
    "The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00092-cf41eb01-2baf-4818-9bc7-d825be42b4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9527777777777777\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(digits_X_train, digits_y_train)\n",
    "\n",
    "perceptron_score = perceptron.score(digits_X_test, digits_y_test)\n",
    "\n",
    "print(f\"Accuracy: {perceptron_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00093-c982ada3-1ad7-436e-a872-b7d18c955846"
   },
   "outputs": [],
   "source": [
    "clf_index.append(\"Perceptron\")\n",
    "clf_results.append(dict(zip(clf_keys, [perceptron_score])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00106-3ecaa8dd-d1c3-4e7a-a2a6-f2c77da4fb75"
   },
   "source": [
    "# 3. Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00111-1173881f-e453-4dc8-bd45-da7f22dd2bfe"
   },
   "source": [
    "## 3.1 Polynomial regression: extending linear models with basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00112-9b872937-16b6-43af-ad4d-bc84e7260b41"
   },
   "source": [
    "One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n",
    "\n",
    "For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2$$\n",
    "If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$$\n",
    "The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new set of features\n",
    "$$z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]$$\n",
    "With this re-labeling of the data, our problem can be written\n",
    "$$\\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5$$\n",
    "We see that the resulting polynomial regression is in the same class of linear models we considered above (i.e. the model is linear in ) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\n",
    "\n",
    "Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00113-bf10facb-a9d7-40d5-881f-140dbb5229b0"
   },
   "source": [
    "<img src=\"../../resources/img/polynomial.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00114-f809e431-247b-4a38-9970-40e9bc79b3d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 65.35122267783325\n",
      "Coefficients: [ 2.72265831e-09  1.08800751e+02 -3.11877460e+02  4.21938205e+02\n",
      "  3.91310073e+02 -1.66531111e+04  1.45163271e+04  5.93019133e+03\n",
      "  4.42673795e+01  6.13001899e+03  9.88062573e+00  2.38937018e+03\n",
      "  1.52485368e+03 -1.29032694e+03  6.62502189e+02 -9.43431456e+02\n",
      " -7.24101051e+03  5.90997706e+03  9.79932135e+03  1.22536527e+03\n",
      "  8.60270079e+02 -1.88326650e+00  3.45951367e+02  1.25466568e+03\n",
      "  1.61897732e+03  1.64705649e+03 -3.53392670e+03 -7.60359401e+03\n",
      "  2.12789918e+03  6.87728645e+02  8.01383901e+02  3.34807102e+03\n",
      " -6.15360849e+03  7.29631034e+03 -2.12219897e+02 -3.58505784e+03\n",
      "  3.69391835e+03  4.74707827e+02 -2.56238551e+02  1.99355292e+04\n",
      " -1.46430010e+04 -8.28659726e+03 -1.39243467e+03 -7.89699555e+03\n",
      " -3.30251051e+03  9.38240672e+04 -1.29517278e+05 -6.18814888e+04\n",
      " -2.52509462e+04 -4.18286890e+04 -7.83704644e+03  4.60161861e+04\n",
      "  3.47403128e+04  6.47107152e+03  2.50104208e+04  4.13987760e+03\n",
      "  1.10995735e+04  1.61835840e+04  1.38485452e+04  7.79546473e+03\n",
      "  1.19861213e+04  4.51398880e+03  5.61986965e+03  3.27934489e+04\n",
      "  4.97924326e+03  1.25887427e+03]\n",
      "R^2: 0.41568772794779973\n"
     ]
    }
   ],
   "source": [
    "ols = LinearRegression()\n",
    "polynomial_features = PolynomialFeatures(degree=2)\n",
    "\n",
    "diabetes_polynomial_X_train = polynomial_features.fit_transform(diabetes_X_train)\n",
    "diabetes_polynomial_X_test = polynomial_features.transform(diabetes_X_test)\n",
    "ols.fit(diabetes_polynomial_X_train, diabetes_y_train)\n",
    " \n",
    "ols_coef = ols.coef_\n",
    "ols_intercept = ols.intercept_\n",
    " \n",
    "ols_score = ols.score(diabetes_polynomial_X_test, diabetes_y_test)\n",
    " \n",
    "print(f\"Intercept: {ols_intercept}\")\n",
    "print(f\"Coefficients: {ols_coef}\")\n",
    "print(f\"R^2: {ols_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "00115-6c2a9eb9-e1ea-4a80-9dcb-a88960d3e11c"
   },
   "outputs": [],
   "source": [
    "reg_index.append(\"PolynomialOLS\")\n",
    "reg_results.append(dict(zip(reg_keys, [ols_intercept] + [None]*(len(reg_keys)-2) + [ols_score])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00116-9517309e-1e06-419d-9e7c-909934b7402a"
   },
   "source": [
    "In some cases it’s not necessary to include higher powers of any single feature, but only the so-called interaction features that multiply together features (with the setting `interaction_only=True`). <br> \n",
    "For example, it's very usefull when dealing with boolean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "00117-081d887f-befa-45fa-a318-d66c98365e99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "polynomial_features = PolynomialFeatures(interaction_only=True)\n",
    "\n",
    "digits_polynomial_X_train = polynomial_features.fit_transform(digits_X_train)\n",
    "digits_polynomial_X_test = polynomial_features.transform(digits_X_test)\n",
    "perceptron.fit(digits_polynomial_X_train, digits_y_train)\n",
    "\n",
    "perceptron_score = perceptron.score(digits_polynomial_X_test, digits_y_test)\n",
    "\n",
    "print(f\"Accuracy: {perceptron_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "00118-8bd061f9-bd10-49bb-9a79-ddd4b36bae17"
   },
   "outputs": [],
   "source": [
    "clf_index.append(\"PolynomialPerceptron\")\n",
    "clf_results.append(dict(zip(clf_keys, [perceptron_score])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00119-93dd9b3b-538c-493f-903e-a940e229de5e"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "00120-7dd8292d-99fe-4611-bd06-91464bcb42c7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>w0</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w6</th>\n",
       "      <th>w7</th>\n",
       "      <th>w8</th>\n",
       "      <th>w9</th>\n",
       "      <th>R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>151.345655</td>\n",
       "      <td>37.900314</td>\n",
       "      <td>-241.966248</td>\n",
       "      <td>542.425753</td>\n",
       "      <td>347.708305</td>\n",
       "      <td>-931.461261</td>\n",
       "      <td>518.044055</td>\n",
       "      <td>163.403535</td>\n",
       "      <td>275.310038</td>\n",
       "      <td>736.189098</td>\n",
       "      <td>48.671125</td>\n",
       "      <td>0.452607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>151.574883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-152.667066</td>\n",
       "      <td>552.694172</td>\n",
       "      <td>303.370551</td>\n",
       "      <td>-81.364834</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-229.258298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>447.918189</td>\n",
       "      <td>29.642354</td>\n",
       "      <td>0.471855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>153.717751</td>\n",
       "      <td>0.401004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.413534</td>\n",
       "      <td>2.326173</td>\n",
       "      <td>0.459577</td>\n",
       "      <td>0.125552</td>\n",
       "      <td>-1.783112</td>\n",
       "      <td>2.124010</td>\n",
       "      <td>3.071387</td>\n",
       "      <td>1.906183</td>\n",
       "      <td>-0.002465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolynomialOLS</th>\n",
       "      <td>65.351223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.415688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                intercept         w0          w1          w2          w3  \\\n",
       "OLS            151.345655  37.900314 -241.966248  542.425753  347.708305   \n",
       "Lasso          151.574883   0.000000 -152.667066  552.694172  303.370551   \n",
       "ElasticNet     153.717751   0.401004    0.000000    3.413534    2.326173   \n",
       "PolynomialOLS   65.351223        NaN         NaN         NaN         NaN   \n",
       "\n",
       "                       w4          w5          w6          w7          w8  \\\n",
       "OLS           -931.461261  518.044055  163.403535  275.310038  736.189098   \n",
       "Lasso          -81.364834   -0.000000 -229.258298    0.000000  447.918189   \n",
       "ElasticNet       0.459577    0.125552   -1.783112    2.124010    3.071387   \n",
       "PolynomialOLS         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "                      w9       R^2  \n",
       "OLS            48.671125  0.452607  \n",
       "Lasso          29.642354  0.471855  \n",
       "ElasticNet      1.906183 -0.002465  \n",
       "PolynomialOLS        NaN  0.415688  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reg_results, index=reg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "00121-a5ef3441-8892-4b64-a936-a4507fcc970b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.952778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolynomialPerceptron</th>\n",
       "      <td>0.986111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Accuracy\n",
       "LogisticRegression    0.972222\n",
       "Perceptron            0.952778\n",
       "PolynomialPerceptron  0.986111"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf_results, index=clf_index)"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "3bf54e1f-4f52-466c-a576-ebceaff56461",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
